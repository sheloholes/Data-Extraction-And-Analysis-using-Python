{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee1b875",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0748a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "589a6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Created\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Input.xlsx.csv\")          # To read input file\n",
    "df['URL_ID'] = df['URL_ID'].astype(str).replace('\\.0', '', regex=True) #For accurate file name\n",
    "print(\"Dataframe Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3c622e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction Started\n",
      "Extraction Complete\n",
      "Time taken for extraction  253.02700448036194\n"
     ]
    }
   ],
   "source": [
    "#This Block of Code Takes Time To Complete\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(\"Extraction Started\")\n",
    "for index, row in df.iterrows():\n",
    "    result=requests.get(row[\"URL\"])\n",
    "    soup=bs4.BeautifulSoup(result.text,'lxml')\n",
    "    try:                                                          #Using Try try block for error handeling\n",
    "        title=soup.select(\"h1\")[0].text\n",
    "        filecontent=soup.select(\".td-post-content\")[0].text\n",
    "    except:\n",
    "        filecontent=\"Ooops... Error 404\"\n",
    "    with open(f\".//output_files//{row['URL_ID']}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(title)\n",
    "        f.write(filecontent)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extraction Complete\")\n",
    "print(\"Time taken for extraction \",end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734b562",
   "metadata": {},
   "source": [
    "# For Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab58a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Complete\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import punkt\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import cmudict\n",
    "# Download if not already downloaded\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('corpus')\n",
    "\n",
    "\n",
    "#Output Dataframe\n",
    "output_df = pd.DataFrame(\n",
    "    columns=['URL_ID','URL','POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "             'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "             'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH',])\n",
    "\n",
    "def count_syllables(word, dic):\n",
    "    return len(dic.inserted(word).split('-'))\n",
    "\n",
    "\n",
    "#Iterating input.csv file's df for reference\n",
    "for index, row in df.iterrows():\n",
    "    output_df.loc[index,\"URL_ID\"]=row[\"URL_ID\"]\n",
    "    output_df.loc[index,\"URL\"]=row[\"URL\"]\n",
    "    with open(f\".//output_files//{row['URL_ID']}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data=f.read()\n",
    "        \n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        # Get sentiment scores\n",
    "        sentiment_scores = sia.polarity_scores(data)\n",
    "        # Positive score\n",
    "        output_df.loc[index,\"POSITIVE SCORE\"] = sentiment_scores['pos']\n",
    "        output_df.loc[index,'NEGATIVE SCORE'] = sentiment_scores['neg']\n",
    "        \n",
    "          \n",
    "        \n",
    "        blob = TextBlob(data)\n",
    "        output_df.loc[index,'POLARITY SCORE'] = blob.sentiment.polarity\n",
    "        output_df.loc[index,'SUBJECTIVITY SCORE'] = blob.sentiment.subjectivity\n",
    "        \n",
    "        \n",
    "        sentences = nltk.sent_tokenize(data)\n",
    "        words = nltk.word_tokenize(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Count total number of words\n",
    "        total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "        # Count total number of sentences\n",
    "        total_sentences = len(sentences)\n",
    "        # Calculate average sentence length\n",
    "        avg_sent_length=total_words / total_sentences\n",
    "        \n",
    "        output_df.loc[index,'AVG SENTENCE LENGTH'] = avg_sent_length\n",
    "        \n",
    "        output_df.loc[index,'AVG NUMBER OF WORDS PER SENTENCE'] = avg_sent_length\n",
    "    \n",
    "        # Initialize pyphen dictionary\n",
    "        dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "        # Define a threshold for complex words (e.g., words with more than three syllables)\n",
    "        complex_word_threshold = 3\n",
    "\n",
    "        # Process text in smaller chunks\n",
    "        chunk_size = 100  # Adjust as needed\n",
    "        complex_word_count = sum(1 for word in words if count_syllables(word, dic) > complex_word_threshold)\n",
    "        total_word_count = len(words)\n",
    "\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = words[i:i+chunk_size]\n",
    "            for word in chunk:\n",
    "                total_word_count += 1\n",
    "                if count_syllables(word, dic) > complex_word_threshold:\n",
    "                    complex_word_count += 1\n",
    "\n",
    "        # Calculate the percentage of complex words\n",
    "        percent_of_complex_words=(complex_word_count / total_word_count) * 100\n",
    "        \n",
    "        output_df.loc[index,'PERCENTAGE OF COMPLEX WORDS'] = percent_of_complex_words\n",
    "        output_df.loc[index,'COMPLEX WORD COUNT'] = complex_word_count\n",
    "        output_df.loc[index,'WORD COUNT'] = total_word_count\n",
    "\n",
    "        FOG_Index = 0.4 * (avg_sent_length + percent_of_complex_words)\n",
    "        \n",
    "        output_df.loc[index,'FOG INDEX']=FOG_Index\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_syllables = sum(count_syllables(word, dic) for word in words)\n",
    "        average_syllables_per_word = total_syllables / total_words\n",
    "        output_df.loc[index,'SYLLABLE PER WORD']=average_syllables_per_word\n",
    "        \n",
    "        \n",
    "        personal_pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\"]\n",
    "\n",
    "        # Count the occurrences of personal pronouns\n",
    "        personal_pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n",
    "        output_df.loc[index,'PERSONAL PRONOUNS']=personal_pronoun_count\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_characters = sum(len(word) for word in words)\n",
    "        average_word_length = total_characters / total_words\n",
    "        output_df.loc[index,'AVG WORD LENGTH']=average_word_length\n",
    "print(\"Analysis Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f91546d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.136034</td>\n",
       "      <td>0.43728</td>\n",
       "      <td>23.25</td>\n",
       "      <td>3.494624</td>\n",
       "      <td>10.697849</td>\n",
       "      <td>23.25</td>\n",
       "      <td>130</td>\n",
       "      <td>3720</td>\n",
       "      <td>1.537097</td>\n",
       "      <td>27</td>\n",
       "      <td>5.139247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.615704</td>\n",
       "      <td>27.04</td>\n",
       "      <td>6.213018</td>\n",
       "      <td>13.301207</td>\n",
       "      <td>27.04</td>\n",
       "      <td>84</td>\n",
       "      <td>1352</td>\n",
       "      <td>1.621302</td>\n",
       "      <td>16</td>\n",
       "      <td>5.097633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.086835</td>\n",
       "      <td>0.459666</td>\n",
       "      <td>17.768116</td>\n",
       "      <td>3.344209</td>\n",
       "      <td>8.44493</td>\n",
       "      <td>17.768116</td>\n",
       "      <td>82</td>\n",
       "      <td>2452</td>\n",
       "      <td>1.504078</td>\n",
       "      <td>12</td>\n",
       "      <td>4.820555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.139706</td>\n",
       "      <td>0.385624</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>3.323699</td>\n",
       "      <td>10.556146</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>92</td>\n",
       "      <td>2768</td>\n",
       "      <td>1.554191</td>\n",
       "      <td>17</td>\n",
       "      <td>5.051301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.139706</td>\n",
       "      <td>0.385624</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>3.323699</td>\n",
       "      <td>10.556146</td>\n",
       "      <td>23.066667</td>\n",
       "      <td>92</td>\n",
       "      <td>2768</td>\n",
       "      <td>1.554191</td>\n",
       "      <td>17</td>\n",
       "      <td>5.051301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>50921</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.077797</td>\n",
       "      <td>0.432445</td>\n",
       "      <td>25.733333</td>\n",
       "      <td>4.145078</td>\n",
       "      <td>11.951364</td>\n",
       "      <td>25.733333</td>\n",
       "      <td>64</td>\n",
       "      <td>1544</td>\n",
       "      <td>1.470207</td>\n",
       "      <td>2</td>\n",
       "      <td>4.839378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.401288</td>\n",
       "      <td>37.84</td>\n",
       "      <td>2.114165</td>\n",
       "      <td>15.981666</td>\n",
       "      <td>37.84</td>\n",
       "      <td>80</td>\n",
       "      <td>3784</td>\n",
       "      <td>1.441332</td>\n",
       "      <td>4</td>\n",
       "      <td>4.731501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>https://insights.blackcoffer.com/what-are-the-...</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.132965</td>\n",
       "      <td>0.455201</td>\n",
       "      <td>27.873239</td>\n",
       "      <td>3.183426</td>\n",
       "      <td>12.422666</td>\n",
       "      <td>27.873239</td>\n",
       "      <td>126</td>\n",
       "      <td>3958</td>\n",
       "      <td>1.497726</td>\n",
       "      <td>32</td>\n",
       "      <td>4.767054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>https://insights.blackcoffer.com/marketing-dri...</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.073452</td>\n",
       "      <td>0.434461</td>\n",
       "      <td>26.915254</td>\n",
       "      <td>3.02267</td>\n",
       "      <td>11.97517</td>\n",
       "      <td>26.915254</td>\n",
       "      <td>96</td>\n",
       "      <td>3176</td>\n",
       "      <td>1.467884</td>\n",
       "      <td>33</td>\n",
       "      <td>4.642317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>https://insights.blackcoffer.com/continued-dem...</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.44689</td>\n",
       "      <td>27.585366</td>\n",
       "      <td>5.835544</td>\n",
       "      <td>13.368364</td>\n",
       "      <td>27.585366</td>\n",
       "      <td>132</td>\n",
       "      <td>2262</td>\n",
       "      <td>1.650752</td>\n",
       "      <td>11</td>\n",
       "      <td>5.271441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID                                                URL  \\\n",
       "0        123  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "1        321  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "2       2345  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "3       4321  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "4        432  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "..       ...                                                ...   \n",
       "109    50921  https://insights.blackcoffer.com/coronavirus-i...   \n",
       "110  51382.8  https://insights.blackcoffer.com/coronavirus-i...   \n",
       "111  51844.6  https://insights.blackcoffer.com/what-are-the-...   \n",
       "112  52306.4  https://insights.blackcoffer.com/marketing-dri...   \n",
       "113  52768.2  https://insights.blackcoffer.com/continued-dem...   \n",
       "\n",
       "    POSITIVE SCORE NEGATIVE SCORE POLARITY SCORE SUBJECTIVITY SCORE  \\\n",
       "0            0.137           0.02       0.136034            0.43728   \n",
       "1            0.171           0.01       0.111801           0.615704   \n",
       "2            0.131          0.047       0.086835           0.459666   \n",
       "3            0.205          0.055       0.139706           0.385624   \n",
       "4            0.205          0.055       0.139706           0.385624   \n",
       "..             ...            ...            ...                ...   \n",
       "109          0.032          0.049       0.077797           0.432445   \n",
       "110          0.067          0.106       0.013231           0.401288   \n",
       "111          0.119          0.023       0.132965           0.455201   \n",
       "112          0.089           0.06       0.073452           0.434461   \n",
       "113          0.147          0.083         0.0561            0.44689   \n",
       "\n",
       "    AVG SENTENCE LENGTH PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                 23.25                    3.494624  10.697849   \n",
       "1                 27.04                    6.213018  13.301207   \n",
       "2             17.768116                    3.344209    8.44493   \n",
       "3             23.066667                    3.323699  10.556146   \n",
       "4             23.066667                    3.323699  10.556146   \n",
       "..                  ...                         ...        ...   \n",
       "109           25.733333                    4.145078  11.951364   \n",
       "110               37.84                    2.114165  15.981666   \n",
       "111           27.873239                    3.183426  12.422666   \n",
       "112           26.915254                     3.02267   11.97517   \n",
       "113           27.585366                    5.835544  13.368364   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE COMPLEX WORD COUNT WORD COUNT  \\\n",
       "0                              23.25                130       3720   \n",
       "1                              27.04                 84       1352   \n",
       "2                          17.768116                 82       2452   \n",
       "3                          23.066667                 92       2768   \n",
       "4                          23.066667                 92       2768   \n",
       "..                               ...                ...        ...   \n",
       "109                        25.733333                 64       1544   \n",
       "110                            37.84                 80       3784   \n",
       "111                        27.873239                126       3958   \n",
       "112                        26.915254                 96       3176   \n",
       "113                        27.585366                132       2262   \n",
       "\n",
       "    SYLLABLE PER WORD PERSONAL PRONOUNS AVG WORD LENGTH  \n",
       "0            1.537097                27        5.139247  \n",
       "1            1.621302                16        5.097633  \n",
       "2            1.504078                12        4.820555  \n",
       "3            1.554191                17        5.051301  \n",
       "4            1.554191                17        5.051301  \n",
       "..                ...               ...             ...  \n",
       "109          1.470207                 2        4.839378  \n",
       "110          1.441332                 4        4.731501  \n",
       "111          1.497726                32        4.767054  \n",
       "112          1.467884                33        4.642317  \n",
       "113          1.650752                11        5.271441  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
